{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, os, re\n",
    "import imgproc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from ResUnet import CRAFT\n",
    "import torch\n",
    "\n",
    "\n",
    "net = CRAFT(input_channel = 3, n_classes = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp = torch.randn((1,3,768,768))\n",
    "\n",
    "x, output_feature = net(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/nas/1_user/eunsung.shin@agilesoda.ai/module/CRAFTS/config/config.py:7: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  self._dict = yaml.load(self._yaml)\n"
     ]
    }
   ],
   "source": [
    "from config import config\n",
    "from dataloader import Dataset\n",
    "\n",
    "cfg = config.Config('./config/config.yml')\n",
    "dataloader = Dataset(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgfile = os.path.join(dataloader.img_folder, dataloader.images_path[0])\n",
    "gtfile = os.path.join(dataloader.gt_folder, dataloader.images_path[0].replace('.jpg', '.txt').replace('.png', '.txt'))\n",
    "print(imgfile)\n",
    "print(gtfile)\n",
    "bboxes, words = dataloader.load_gt(gtfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = imgproc.loadImage(imgfile)\n",
    "\n",
    "img_resized, target_ratio, size_heatmap = imgproc.resize_aspect_ratio(image, 2380, interpolation=cv2.INTER_LINEAR, mag_ratio= 2.5)\n",
    "ratio_h = ratio_w = 1 / target_ratio\n",
    "\n",
    "# preprocessing\n",
    "x = imgproc.normalizeMeanVariance(img_resized)\n",
    "x = torch.from_numpy(x).permute(2, 0, 1)    # [h, w, c] to [c, h, w]\n",
    "x = Variable(x.unsqueeze(0)) \n",
    "\n",
    "result, output_feature = net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STR_inputs = torch.cat([result[:,:2,:,:], output_feature], axis = 1).permute(0,2,3,1).cuda()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STR_inputs.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def crop_image_by_bbox(image, box, word):\n",
    "\n",
    "    rot_angle = None\n",
    "    match = re.findall(r'\\[ROT[0-9]+\\]', word)\n",
    "    if len(match) > 0  :\n",
    "        rot_angle = int(re.findall(r'\\d+', match[0])[0])\n",
    "    word = word.replace('[UNK]', '*')\n",
    "    word = re.sub(r'[UNK[0-9]+]|[ROT[0-9]+]', '', word)\n",
    "    if len(box) == 4 :\n",
    "        w = (int)(np.linalg.norm(box[0] - box[1]))\n",
    "        h = (int)(np.linalg.norm(box[0] - box[3]))\n",
    "        width = w\n",
    "        height = h\n",
    "        print(width, height)\n",
    "        if h > w * 1.5 and len(word) != 1 :\n",
    "            width = h\n",
    "            height = w\n",
    "            # [ROT90]일 때\n",
    "            if rot_angle == 90 or rot_angle == None :\n",
    "                M = cv2.getPerspectiveTransform(np.float32(box),\n",
    "                                np.float32(np.array([[0, height], [0,0], [width, 0], [width, height]])))\n",
    "            if rot_angle == 270 or rot_angle == 0 :\n",
    "                M = cv2.getPerspectiveTransform(np.float32(box),\n",
    "                                            np.float32(np.array([[width, 0], [width, height], [0, height], [0, 0]])))\n",
    "        else:\n",
    "            M = cv2.getPerspectiveTransform(np.float32(box),\n",
    "                                            np.float32(np.array([[0, 0], [width, 0], [width, height], [0, height]])))\n",
    "\n",
    "        warped = cv2.warpPerspective(image, M, (width, height))\n",
    "        return warped, M\n",
    "    else : \n",
    "        # polygon(>4)\n",
    "        pts = np.int32(box)\n",
    "        rect = cv2.boundingRect(pts)\n",
    "        x,y,w,h = rect\n",
    "        x,y = max(0, x), max(0, y)\n",
    "        x,y = min(x, image.shape[1]), min(y, image.shape[0])\n",
    "        cropped = np.array(image)[y:y+h, x:x+w]\n",
    "        pts = pts - pts.min(axis = 0)\n",
    "\n",
    "        mask = np.zeros(cropped.shape[:2], np.uint8)\n",
    "        cv2.drawContours(mask, [pts], -1, (255,255,255), -1, cv2.LINE_AA)\n",
    "        cropped_black_bg = cv2.bitwise_and(cropped, cropped, mask=mask)\n",
    "        bg = np.ones_like(cropped, np.uint8)*255\n",
    "        cv2.bitwise_not(bg,bg, mask = mask)\n",
    "        cropped_white_bg = bg + cropped_black_bg\n",
    "\n",
    "        return cropped_white_bg, (x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_feature, _ = crop_image_by_bbox(np.array(STR_inputs[0].detach()), bboxes[0], words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(dataloader,\n",
    "                                              batch_size=cfg.BATCH_SIZE,\n",
    "                                              shuffle=False,\n",
    "                                              num_workers=cfg.WORKERS,\n",
    "                                              drop_last=True,\n",
    "                                              pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:04,  4.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 32, 27])\n",
      "torch.Size([4, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:12,  6.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 32, 27])\n",
      "torch.Size([4, 32])\n",
      "torch.Size([4, 32, 27])\n",
      "torch.Size([4, 32])\n",
      "torch.Size([4, 32, 27])\n",
      "torch.Size([4, 32])\n",
      "torch.Size([4, 32, 27])\n",
      "torch.Size([4, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:20,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 32, 27])\n",
      "torch.Size([4, 32])\n",
      "torch.Size([4, 32, 27])\n",
      "torch.Size([4, 32])\n",
      "torch.Size([4, 32, 27])\n",
      "torch.Size([4, 32])\n",
      "torch.Size([4, 32, 27])\n",
      "torch.Size([4, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:21,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 32, 27])\n",
      "torch.Size([4, 32])\n",
      "torch.Size([4, 32, 27])\n",
      "torch.Size([4, 32])\n",
      "torch.Size([4, 32, 27])\n",
      "torch.Size([4, 32])\n",
      "torch.Size([4, 32, 27])\n",
      "torch.Size([4, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [00:22,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 32, 27])\n",
      "torch.Size([4, 32])\n",
      "torch.Size([4, 32, 27])\n",
      "torch.Size([4, 32])\n",
      "torch.Size([4, 32, 27])\n",
      "torch.Size([4, 32])\n",
      "torch.Size([4, 32, 27])\n",
      "torch.Size([4, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18it [00:25,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 32, 27])\n",
      "torch.Size([4, 32])\n",
      "torch.Size([4, 32, 27])\n",
      "torch.Size([4, 32])\n",
      "torch.Size([4, 32, 27])\n",
      "torch.Size([4, 32])\n",
      "torch.Size([4, 32, 27])\n",
      "torch.Size([4, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24it [00:28,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 32, 27])\n",
      "torch.Size([4, 32])\n",
      "torch.Size([4, 32, 27])\n",
      "torch.Size([4, 32])\n",
      "torch.Size([4, 32, 27])\n",
      "torch.Size([4, 32])\n",
      "torch.Size([4, 32, 27])\n",
      "torch.Size([4, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:28,  1.15s/it]\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "Caught UnboundLocalError in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 198, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/jovyan/nas/1_user/eunsung.shin@agilesoda.ai/module/CRAFTS/dataloader.py\", line 429, in __getitem__\n    return self.pull_item(index)\n  File \"/home/jovyan/nas/1_user/eunsung.shin@agilesoda.ai/module/CRAFTS/dataloader.py\", line 653, in pull_item\n    og_image, character_bboxes, words, confidence_mask, confidences, word_bboxes, og_shape = self.load_image_gt_and_confidencemask(index)\n  File \"/home/jovyan/nas/1_user/eunsung.shin@agilesoda.ai/module/CRAFTS/dataloader.py\", line 643, in load_image_gt_and_confidencemask\n    pursedo_bboxes, confidence = self.inference_pursedo_bboxes( image, word_bboxes[i], words[i], viz=self.viz)\n  File \"/home/jovyan/nas/1_user/eunsung.shin@agilesoda.ai/module/CRAFTS/dataloader.py\", line 541, in inference_pursedo_bboxes\n    word_image, MM = self.crop_image_by_bbox(image, word_bbox, word)\n  File \"/home/jovyan/nas/1_user/eunsung.shin@agilesoda.ai/module/CRAFTS/dataloader.py\", line 518, in crop_image_by_bbox\n    warped = cv2.warpPerspective(image, M, (width, height))\nUnboundLocalError: local variable 'M' referenced before assignment\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-49ceb854b0c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgh_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgah_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mori_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mori_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_bboxes_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords_length_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_length_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1166\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1167\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1083\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1085\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1111\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1112\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    426\u001b[0m             \u001b[0;31m# have message field\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: Caught UnboundLocalError in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 198, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/jovyan/nas/1_user/eunsung.shin@agilesoda.ai/module/CRAFTS/dataloader.py\", line 429, in __getitem__\n    return self.pull_item(index)\n  File \"/home/jovyan/nas/1_user/eunsung.shin@agilesoda.ai/module/CRAFTS/dataloader.py\", line 653, in pull_item\n    og_image, character_bboxes, words, confidence_mask, confidences, word_bboxes, og_shape = self.load_image_gt_and_confidencemask(index)\n  File \"/home/jovyan/nas/1_user/eunsung.shin@agilesoda.ai/module/CRAFTS/dataloader.py\", line 643, in load_image_gt_and_confidencemask\n    pursedo_bboxes, confidence = self.inference_pursedo_bboxes( image, word_bboxes[i], words[i], viz=self.viz)\n  File \"/home/jovyan/nas/1_user/eunsung.shin@agilesoda.ai/module/CRAFTS/dataloader.py\", line 541, in inference_pursedo_bboxes\n    word_image, MM = self.crop_image_by_bbox(image, word_bbox, word)\n  File \"/home/jovyan/nas/1_user/eunsung.shin@agilesoda.ai/module/CRAFTS/dataloader.py\", line 518, in crop_image_by_bbox\n    warped = cv2.warpPerspective(image, M, (width, height))\nUnboundLocalError: local variable 'M' referenced before assignment\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for index, (images, gh_label, gah_label, mask, ori_x, ori_y, word_bboxes_batch, words_batch, words_length_batch) in tqdm(enumerate(data_loader)):\n",
    "    print(words_batch.shape)\n",
    "    print(words_length_batch.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, region_scores_torch, affinity_scores_torch, confidence_mask_torch, orientation_x_torch, orientation_y_torch, cropped_word_bboxes, encoded_cropped_words, words_length = dataloader.pull_item( 29)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import AttnLabelConverter\n",
    "import data_utils\n",
    "\n",
    "def load_character_list(path) :\n",
    "    with open(path, 'r') as f :\n",
    "        character_list = f.read()\n",
    "        \n",
    "    return character_list\n",
    "character_list = load_character_list('./char_dicts/charset-kor-sp.txt')\n",
    "converter = AttnLabelConverter(character_list)\n",
    "\n",
    "pad_batch, text_batch, length_batch = [], [], []\n",
    "            \n",
    "idx = 0 \n",
    "STR_input = STR_inputs[idx]\n",
    "word_bboxes = word_bboxes_batch[idx]\n",
    "words = words_batch[idx]\n",
    "words_length = words_length_batch[idx]\n",
    "decoded_words = converter.decode(words, words_length)\n",
    "\n",
    "feature_ls = []\n",
    "for word_bbox, word, decoded_word, word_length in zip(word_bboxes, words ,decoded_words, words_length) :\n",
    "    if word_length != 1  :\n",
    "        cropFeature, _ = data_utils.crop_image_by_bbox(STR_input.detach().numpy(), word_bbox, decoded_word)\n",
    "        feature_ls.append(cropFeature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_ls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "cv2.resize(STR_input.detach().numpy(), (800, 1200), Image.BICUBIC).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STR_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegionResizeNormalize(object):\n",
    "\n",
    "    def __init__(self, size, interpolation=Image.BICUBIC):\n",
    "        self.size = size\n",
    "        self.interpolation = interpolation\n",
    "        self.toTensor = transforms.ToTensor()\n",
    "        \n",
    "    def __call__(self, img):\n",
    "        img = cv2.resize(img, self.size, Image.BICUBIC)\n",
    "        img = self.toTensor(img)\n",
    "        img.sub_(0.5).div_(0.5)\n",
    "        return img\n",
    "    \n",
    "class RegionNormalizePAD(object):\n",
    "    def __init__(self, max_size, PAD_type='right'):\n",
    "        self.toTensor = transforms.ToTensor()\n",
    "        self.max_size = max_size\n",
    "        self.max_width_half = math.floor(max_size[2] / 2)\n",
    "        self.PAD_type = PAD_type\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img = self.toTensor(img)\n",
    "        img.sub_(0.5).div_(0.5)\n",
    "        c, h, w = img.size()\n",
    "        Pad_img = torch.FloatTensor(*self.max_size).fill_(0) \n",
    "        Pad_img[:, :, :w] = img  # right pad\n",
    "        if self.max_size[2] != w:  # add border Pad\n",
    "            Pad_img[:, :, w:] = img[ :, :, w - 1].unsqueeze(2).expand(c, h, self.max_size[2] - w)\n",
    "\n",
    "        return Pad_img\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class RegionAlignCollate(object) :\n",
    "    def __init__(self, imgH=32, imgW=100, keep_ratio_with_pad=False):\n",
    "        self.imgH = imgH\n",
    "        self.imgW = imgW\n",
    "        self.keep_ratio_with_pad = keep_ratio_with_pad\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "\n",
    "        batch = filter(lambda x: x is not None, batch)\n",
    "        \n",
    "        if self.keep_ratio_with_pad :\n",
    "            resized_max_w = self.imgW\n",
    "            input_channel = batch[0].shape[-1]\n",
    "            transform = RegionNormalizePAD((input_channel, self.imgH, resized_max_w))\n",
    "\n",
    "            resized_images = []\n",
    "            for image in batch :\n",
    "                if math.ceil(self.imgH * ratio) > self.imgW:\n",
    "                    resized_w = self.imgW\n",
    "                else:\n",
    "                    resized_w = math.ceil(self.imgH * ratio)\n",
    "\n",
    "                resized_image = cv2.resize(image, (resized_w, self.imgH), Image.BICUBIC)\n",
    "                resized_images.append(transform(resized_image))\n",
    "                \n",
    "            image_tensors = torch.cat([t.unsqueeze(0) for t in resized_images], 0)    \n",
    "            \n",
    "        else :\n",
    "            transform = RegionResizeNormalize((self.imgW, self.imgH))\n",
    "            image_tensors = [transform(image) for image in batch]\n",
    "            image_tensors = torch.cat([t.unsqueeze(0) for t in image_tensors], 0)\n",
    "            \n",
    "        return image_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding = RegionAlignCollate(imgH=16, imgW=64, keep_ratio_with_pad = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding(feature_ls).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropFeature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "tensorfy = transforms.ToTensor()\n",
    "\n",
    "tensorfy(cropFeature).sub_(0.5).div_(0.5).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pil = Image.open('/home/jovyan/nas/2_public_data/aihub_wildscene_labeled/image/BC255C8EDB6756B8E651FCFF3A43B61C.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.tile(np.array(pil), (3,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.resize(np.array(pil), (50,30)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Tensor([1,2, float('nan')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
