{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.config import Config\n",
    "from dataloader import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/nas/1_user/eunsung.shin@agilesoda.ai/module/CRAFTS/config/config.py:7: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  self._dict = yaml.load(self._yaml)\n"
     ]
    }
   ],
   "source": [
    "cfg = Config('./config/config.yml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Configurations : \n",
      "------------------------------\n",
      "EXP_NAME: ''                                                                     # Where to store logs and models\n",
      "SAVED_MODEL_PATH: '/home/jovyan/nas/1_user/eunsung.shin@agilesoda.ai/module/CRAFTS/saved_models/TPS-SimpleResNet-BiLSTM-Attn-Seed123/CRAFTS15.pth' #'/home/jovyan/nas/1_user/eunsung.shin@agilesoda.ai/module/CRAFTS/saved_models/TPS-ResNet-BiLSTM-Attn-Seed123/CRAFTS28.pth'\n",
      "MODE: 1                                                                          # 1: train, 2: test\n",
      "MODEL: 1                                                                         # 1: CRAFTS, 2: CRAFT, 3: STR\n",
      "SEED: 123                                                                       # random seed\n",
      "GPU: ['2']                                                                         # list of gpu ids\n",
      "WORKERS: 16                                                                      # number of data loading workers\n",
      "CUDA: True # 현재, test_crafts는 무조건 cpu모드로 돌아감. line 266\n",
      "\n",
      "DATA_PATH: '/home/jovyan/nas/2_public_data/TwinReader/std_data/textdetection_v2_large/AIHUB_wild_scene/'             # Path to data loader; should have 'image' and 'label_txt' folder\n",
      "\n",
      "STD_CONFIG_PATH: './config/detection.yaml'                                       # Path to STD configuration file # 아직 사용할 필요 없음(09/23일 기준)\n",
      "STR_CONFIG_PATH: './config/recognition.yaml'                                     # Path to STR configuration file\n",
      "\n",
      "LR: 1e-5                                                                         # initial learning rate\n",
      "MOMENTUM: 1e-3                                                                   # Momentum value for optim\n",
      "WEIGHT_DECAY: 5e-4                                                               # Weight decay for SGD\n",
      "GAMMA: 0.99                                                                      # Gamma update for SGD\n",
      "BATCH_SIZE: 2                                                                    # Batch Size\n",
      "MAX_EPOCH: 3000                                                                  # Number of training teration\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cfg.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls =[]\n",
    "ls.extend([1])\n",
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = Dataset(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(6500) :\n",
    "    gtfile = os.path.join(dataloader.gt_folder, dataloader.images_path[i].replace('.jpg', '.txt'))\n",
    "    with open(gtfile, 'r', encoding='utf-8-sig') as f :\n",
    "        data = f.read().split('\\n')\n",
    "        for d in data :\n",
    "            if '[ROT90' in d :\n",
    "                print(i)\n",
    "                break\n",
    "#             if len(d.split('\\t')) > 9 :\n",
    "#                 print(i)\n",
    "#                 break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "i = 3\n",
    "gtfile = os.path.join(dataloader.gt_folder, dataloader.images_path[i].replace('.jpg', '.txt'))\n",
    "imgfile = os.path.join(dataloader.img_folder, dataloader.images_path[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes, words = dataloader.load_gt(gtfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "\n",
    "img = Image.open(imgfile)\n",
    "imgdraw = ImageDraw.Draw(img)\n",
    "\n",
    "for bbox in bboxes :\n",
    "    imgdraw.polygon(bbox.flatten().tolist(), fill = None, outline = (0,255,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(dataloader.crop_image_by_bbox(np.array(img), bboxes[5], words[1])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader.inference_pursedo_bboxes(np.array(img), bboxes[3], words[3], viz = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coordinates import sort_rectangle_custom\n",
    "box = sort_rectangle_custom(box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box = bboxes[3]\n",
    "word = words[3]\n",
    "\n",
    "box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = (int)(np.linalg.norm(box[0] - box[1]))\n",
    "h = (int)(np.linalg.norm(box[0] - box[3]))\n",
    "\n",
    "w,h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rect = cv2.boundingRect(box)\n",
    "x,y,w,h = rect\n",
    "\n",
    "box = np.array([[x,y],\n",
    "                [x+w, y],\n",
    "                [x+w, y+h],\n",
    "                [x, y+h]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "w = (int)(np.linalg.norm(box[0] - box[1]))\n",
    "h = (int)(np.linalg.norm(box[0] - box[3]))\n",
    "\n",
    "width = w\n",
    "height = h\n",
    "# if h > w * 1.5:\n",
    "#     width = h\n",
    "#     height = w\n",
    "#     # [ROT90]일 때\n",
    "#     M = cv2.getPerspectiveTransform(np.float32(box),\n",
    "#                                     np.float32(np.array([[0, height], [0,0], [width, 0], [width, height]])))\n",
    "# #     [ROT270]일 때\n",
    "#     M = cv2.getPerspectiveTransform(np.float32(box),\n",
    "#                                                 np.float32(np.array([[width, 0], [width, height], [0, height], [0, 0]])))\n",
    "# else :\n",
    "#     M = cv2.getPerspectiveTransform(np.float32(box),\n",
    "#                                     np.float32(np.array([[0, 0], [width, 0], [width, height], [0, height]])))\n",
    "rot_angle = 90\n",
    "if h > w * 1.5 and len(word) != 1 :\n",
    "    width = h\n",
    "    height = w\n",
    "    # [ROT90]일 때\n",
    "#     if rot_angle == 90 or rot_angle == None :\n",
    "#         M = cv2.getPerspectiveTransform(np.float32(box),\n",
    "#                         np.float32(np.array([[0, height], [0,0], [width, 0], [width, height]])))\n",
    "#     elif rot_angle == 270 or rot_angle == 0 :\n",
    "    M = cv2.getPerspectiveTransform(np.float32(box),\n",
    "                                np.float32(np.array([[width, 0], [width, height], [0, height], [0, 0]])))\n",
    "#     else :\n",
    "#     M = cv2.getPerspectiveTransform(np.float32(box),\n",
    "#                         np.float32(np.array([[0, height], [0,0], [width, 0], [width, height]])))\n",
    "# else:\n",
    "#     M = cv2.getPerspectiveTransform(np.float32(box),\n",
    "#                                     np.float32(np.array([[0, 0], [width, 0], [width, height], [0, height]])))\n",
    "\n",
    "warped = cv2.warpPerspective(np.array(img), M, (width, height))\n",
    "\n",
    "plt.imshow(warped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# polygon(>4)\n",
    "image = np.array(img)\n",
    "pts = np.int32(box)\n",
    "rect = cv2.boundingRect(pts)\n",
    "x,y,w,h = rect\n",
    "x,y = max(0, x), max(0, y)\n",
    "x,y = min(x, image.shape[1]), min(y, image.shape[0])\n",
    "cropped = np.array(image)[y:y+h, x:x+w]\n",
    "pts = pts - pts.min(axis = 0)\n",
    "\n",
    "mask = np.zeros(cropped.shape[:2], np.uint8)\n",
    "cv2.drawContours(mask, [pts], -1, (255,255,255), -1, cv2.LINE_AA)\n",
    "cropped_black_bg = cv2.bitwise_and(cropped, cropped, mask=mask)\n",
    "bg = np.ones_like(cropped, np.uint8)*255\n",
    "cv2.bitwise_not(bg,bg, mask = mask)\n",
    "cropped_white_bg = bg + cropped_black_bg\n",
    "\n",
    "plt.imshow(cropped_white_bg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_bboxes, confidence = dataloader.inference_pursedo_bboxes(np.array(img), bboxes[0], words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(imgfile)\n",
    "imgdraw = ImageDraw.Draw(img)\n",
    "\n",
    "for bbox in char_bboxes :\n",
    "    imgdraw.polygon(bbox.flatten().tolist(), fill = None, outline = (0,255,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import coordinates \n",
    "\n",
    "char_bboxes = [coordinates.sort_rectangle_custom(bbox) for bbox in char_bboxes][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warped_ls = []\n",
    "for box in char_bboxes :\n",
    "    plt.show()\n",
    "    w = (int)(np.linalg.norm(box[0] - box[1]))\n",
    "    h = (int)(np.linalg.norm(box[0] - box[3]))\n",
    "    if h > w * 1.5:\n",
    "        width = h\n",
    "        height = w\n",
    "    M = cv2.getPerspectiveTransform(np.float32(box),\n",
    "                                                np.float32(np.array([[0, 0], [width, 0], [width, height], [0, height]])))\n",
    "    warped = cv2.warpPerspective(np.array(img), M, (width, height))\n",
    "    warped_ls.append(warped)\n",
    "    plt.imshow(warped)\n",
    "    print(warped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (120, 20))\n",
    "plt.imshow(np.concatenate(warped_ls, -2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(gtfile, 'r', encoding = 'utf-8-sig') as f :\n",
    "    data = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "found = None\n",
    "for i, file in tqdm(enumerate(dataloader.images_path)):\n",
    "    with open(os.path.join(dataloader.gt_folder,file.replace('.jpg', '.txt').replace('.png', '.txt')), 'r', encoding = 'utf-8-sig') as f :\n",
    "        data= f.read().split('\\n')\n",
    "        \n",
    "        for d in data :\n",
    "            if len(d.split('\\t'))> 9 :\n",
    "                found = True\n",
    "        if found :\n",
    "            print(i)\n",
    "        found = False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, region_scores_torch, affinity_scores_torch, confidence_mask_torch, orientation_x_torch, orientation_y_torch, cropped_word_bboxes, encoded_cropped_words, words_length = dataloader.pull_item(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_utils\n",
    "from dataset_CRAFTS import RegionAlignCollate\n",
    "from utils import AttnLabelConverter\n",
    "\n",
    "def load_character_list(path) :\n",
    "    with open(path, 'r') as f :\n",
    "        character_list = f.read()\n",
    "        \n",
    "    return character_list\n",
    "\n",
    "img = image.permute(1,2,0).detach().numpy()\n",
    "character_list = load_character_list('./char_dicts/charset-kor-sp.txt')\n",
    "converter = AttnLabelConverter(character_list)\n",
    "\n",
    "words = encoded_cropped_words\n",
    "word_bboxes = cropped_word_bboxes\n",
    "words_length = words_length\n",
    "decoded_words = converter.decode(words, words_length)\n",
    "\n",
    "\n",
    "feature_batch, text_batch, length_batch = [],[],[]\n",
    "\n",
    "padding = RegionAlignCollate(imgH=16, imgW=64, keep_ratio_with_pad = False)\n",
    "for word_bbox, word, decoded_word, word_length in zip(word_bboxes, words, decoded_words, words_length) :\n",
    "\n",
    "    if word_length != 1  :\n",
    "        cropFeature, _ = data_utils.crop_image_by_bbox(img, word_bbox*2, decoded_word)\n",
    "#                         xmin, xmax = int(torch.min(word_bbox[:,0])), int(torch.max(word_bbox[:,0]))\n",
    "#                         ymin, ymax = int(torch.min(word_bbox[:,1])), int(torch.max(word_bbox[:,1]))\n",
    "#                         if xmax - xmin == 0 or ymax - ymin == 0 :\n",
    "#                             print('found zero-size image...skipping.')\n",
    "#                             print(word_bbox)\n",
    "#                             continue\n",
    "#                         cropped = STR_input[:,ymin:ymax, xmin:xmax]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#                         try : \n",
    "        feature_batch.append(cropFeature)\n",
    "#         text_batch.append(word.unsqueeze(0))\n",
    "#         length_batch.append(word_length.unsqueeze(0))\n",
    "#                         except :\n",
    "#                             print(word_bbox)\n",
    "#                             print('idx : ', index)\n",
    "\n",
    "pad_batch = padding(feature_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(pad_batch[3].permute(1,2,0).detach()*255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imgproc import cvt2HeatmapImg\n",
    "plt.figure(figsize = (15, 15))\n",
    "plt.imshow(cvt2HeatmapImg(np.array(region_scores_torch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imgproc import cvt2HeatmapImg\n",
    "plt.figure(figsize = (15, 15))\n",
    "plt.imshow(cvt2HeatmapImg(np.array(affinity_scores_torch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 15))\n",
    "plt.imshow(cvt2HeatmapImg(np.array(orientation_x_torch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 15))\n",
    "plt.imshow(cvt2HeatmapImg(np.array(orientation_y_torch)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Orientation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def scale_orientation(orientation_x, orientation_y, region_scores) :\n",
    "    orientation_region = np.arctan(np.array(orientation_y) / np.array(orientation_x) )*255.\n",
    "    mask = np.zeros_like(orientation_x)\n",
    "    \n",
    "    mask[np.isnan(orientation_region)] = 0\n",
    "    mask[~np.isnan(orientation_region)] = 255\n",
    "    return np.uint8(orientation_region) , np.uint8(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "orientation_x = np.array(orientation_x_torch)*255.\n",
    "orientation_y = np.array(orientation_y_torch)*255.\n",
    "region_score = np.array(region_scores_torch)*255.\n",
    "\n",
    "orientation_h, mask = scale_orientation(orientation_x, orientation_y, region_score)\n",
    "\n",
    "\n",
    "# orientation_s\n",
    "orientation_s = region_score.copy()\n",
    "orientation_s[orientation_s <255.*0.2] = 0\n",
    "orientation_s = np.uint8(orientation_s)\n",
    "\n",
    "# orientation_v\n",
    "scale_region = orientation_s.copy()\n",
    "scale_region[scale_region<255.*0.3] = 0\n",
    "orientation_v = scale_region\n",
    "\n",
    "\n",
    "hsv = cv2.merge([orientation_h, orientation_s, orientation_v])\n",
    "\n",
    "hsv = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)\n",
    "\n",
    "\n",
    "plt.figure(figsize = (15,15))\n",
    "plt.imshow(hsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, torch\n",
    "img = torch.randn((1920,1440, 130))\n",
    "w = (int)(np.linalg.norm(box[0] - box[1]))\n",
    "h = (int)(np.linalg.norm(box[0] - box[3]))\n",
    "width = w\n",
    "height = h\n",
    "if h > w * 1.5:\n",
    "    width = h\n",
    "    height = w\n",
    "    # [ROT90]일 때\n",
    "    M = cv2.getPerspectiveTransform(np.float32(box),\n",
    "                                    np.float32(np.array([[0, height], [0,0], [width, 0], [width, height]])))\n",
    "#     [ROT270]일 때\n",
    "    M = cv2.getPerspectiveTransform(np.float32(box),\n",
    "                                                np.float32(np.array([[width, 0], [width, height], [0, height], [0, 0]])))\n",
    "else :\n",
    "    M = cv2.getPerspectiveTransform(np.float32(box),\n",
    "                                    np.float32(np.array([[0, 0], [width, 0], [width, height], [0, height]])))\n",
    "\n",
    "warped = cv2.warpPerspective(np.array(img), M, (width, height))\n",
    "\n",
    "# plt.imshow(warped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(warped[:,:,:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_cropped_words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(dataloader,\n",
    "                                              batch_size=cfg.BATCH_SIZE,\n",
    "                                              shuffle=False,\n",
    "                                              num_workers=cfg.WORKERS,\n",
    "                                              drop_last=True,\n",
    "                                              pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "for index, (images, gh_label, gah_label, mask, ori_x, ori_y, word_bboxes_batch, words_batch, words_length_batch) in tqdm(enumerate(data_loader)):\n",
    "    print(words_batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_length_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "class AttnLabelConverter(object):\n",
    "    \"\"\" Convert between text-label and text-index \"\"\"\n",
    "\n",
    "    def __init__(self, character):\n",
    "        # character (str): set of the possible characters.\n",
    "        # [GO] for the start token of the attention decoder. [s] for end-of-sentence token.\n",
    "        list_token = ['[GO]', '[s]', '[UNK]']  # ['[s]','[UNK]','[PAD]','[GO]']\n",
    "        list_character = list(character)\n",
    "        self.character = list_token + list_character\n",
    "        self.stop_token = list_token[-2]\n",
    "\n",
    "        self.dict = {}\n",
    "        for i, char in enumerate(self.character):\n",
    "            # print(i, char)\n",
    "            self.dict[char] = i\n",
    "\n",
    "    def encode(self, text, batch_max_length=25):\n",
    "        \"\"\" convert text-label into text-index.\n",
    "        input:\n",
    "            text: text labels of each image. [batch_size]\n",
    "            batch_max_length: max length of text label in the batch. 25 by default\n",
    "\n",
    "        output:\n",
    "            text : the input of attention decoder. [batch_size x (max_length+2)] +1 for [GO] token and +1 for [s] token.\n",
    "                text[:, 0] is [GO] token and text is padded with [GO] token after [s] token.\n",
    "            length : the length of output of attention decoder, which count [s] token also. [3, 7, ....] [batch_size]\n",
    "        \"\"\"\n",
    "        text = [re.sub(r'[UNK[0-9]+]|[ROT[0-9]+]', '', t).strip() for t in text]\n",
    "#         text = [t.strip() for t in text]\n",
    "        length = [len(s) + 1 for s in text]  # +1 for [s] at end of sentence.\n",
    "        # batch_max_length = max(length) # this is not allowed for multi-gpu setting\n",
    "        batch_max_length += 1\n",
    "        # additional +1 for [GO] at first step. batch_text is padded with [GO] token after [s] token.\n",
    "        batch_text = torch.zeros((len(text), batch_max_length + 1), dtype=torch.int64)\n",
    "        for i, t in enumerate(text):\n",
    "            text = list(t)\n",
    "            text.append(self.stop_token)\n",
    "            text = [self.dict[char] if char in self.dict\\\n",
    "                                    else self.dict['[UNK]']\\\n",
    "                                    for char in text ]\n",
    "            batch_text[i][1:1 + len(text)] = torch.LongTensor(text)  # batch_text[:, 0] = [GO] token\n",
    "\n",
    "        return (batch_text, np.array(length))\n",
    "\n",
    "    def decode(self, text_index, length):\n",
    "        \"\"\" convert text-index into text-label. \"\"\"\n",
    "        texts = []\n",
    "        for index, l in enumerate(length):\n",
    "            text = ''.join([self.character[i] for i in text_index[index, :]])\n",
    "            texts.append(text)\n",
    "        return texts\n",
    "\n",
    "def load_character_list(path) :\n",
    "    with open(path, 'r') as f :\n",
    "        character_list = f.read()\n",
    "        \n",
    "    return character_list\n",
    "character_list = load_character_list('./char_dicts/charset-kor-sp.txt')\n",
    "converter = AttnLabelConverter(character_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(''.join(converter.character)) - 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.imshow(images[0].permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = re.sub(r'[UNK[0-9]+]|[ROT[0-9]+]', '', text)\n",
    "text = [t.strip() for t in text]\n",
    "length = [len(s) + 1 for s in text] \n",
    "\n",
    "length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp_batch_text, samp_length = converter.encode(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_image, character_bboxes, words, confidence_mask, confidences, word_bboxes, og_shape = dataloader.load_image_gt_and_confidencemask(0)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "word_bboxes = word_bboxes_batch[i]\n",
    "words = words_batch[idx]\n",
    "words_length = words_length_batch[idx]\n",
    "\n",
    "for word_bbox, word, word_length in zip(word_bboxes, words, words_length) :\n",
    "                    \n",
    "    if word_length != 1  :\n",
    "        data_utils.crop_image_by_bbox()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter.decode(words_batch[0], words_length_batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "converter.decode(words_batch[0], words_length_batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "samp= '[ROT0]sdfsd'\n",
    "\n",
    "samp = re.sub(r'[UNK[0-9]+]|[ROT[0-9]+]', '', samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.choice()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isInterleave(s1: str, s2: str, s3: str) -> bool:\n",
    "    s1 = list(s1)\n",
    "    s2 = list(s2)\n",
    "\n",
    "    for char in s3 :\n",
    "        if char == s1[0] :\n",
    "            s1.pop(0)\n",
    "        elif char == s2[0] :\n",
    "            s2.pop(0)\n",
    "        else :\n",
    "            return False\n",
    "        print(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isInterleave(s1: str, s2: str, s3: str) -> bool:\n",
    "    i,j = 0,0\n",
    "    for char in s3 :\n",
    "        if s1[i] == char :\n",
    "            i+=1\n",
    "        elif s2[j] == char :\n",
    "            j+=1\n",
    "        else :\n",
    "            return False\n",
    "        if i == len(s1) or j == len(s2) :\n",
    "            return False\n",
    "            \n",
    "        print(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isInterleave(s1: str, s2: str, s3: str) -> bool:\n",
    "    s3 = list(s3)\n",
    "    for i in range(len(s1)) :\n",
    "        char = s3.pop(0)\n",
    "        if s1[i] == char or s2[i] == char :\n",
    "            continue\n",
    "        else :\n",
    "            return False\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = 'aabcc'\n",
    "s2 = 'dbbca'\n",
    "s3 = 'aadbbcbcac'\n",
    "isInterleave(s1,s2,s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
